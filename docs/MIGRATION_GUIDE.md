# Migration Guide: Polars to Polaroid

A comprehensive guide to migrate your existing Polars code to Polaroid.

## üìã Table of Contents

1. [Why Migrate](#-why-migrate)
2. [Version-Specific Migrations](#-version-specific-migrations)
   - [v0.53.0: Adaptive Streaming Sources](#v0530-adaptive-streaming-sources-january-2026)
3. [General Polars ‚Üí Polaroid Migration](#-compatibility-overview)

---

## üÜï v0.53.0: Adaptive Streaming Sources (January 2026)

### What's New

Polaroid v0.53.0 introduces **Generic Adaptive Streaming Sources** with support for:
- **CSV** with adaptive chunking
- **Cloud Storage**: S3, Azure Blob, Google Cloud Storage  
- **Databases**: DynamoDB, PostgreSQL, MySQL
- **Streaming**: Apache Kafka
- **HTTP/REST APIs** with automatic pagination
- **Filesystem** with memory mapping

### Quick Migration

#### From Standard Polars CSV

**Before:**
```python
import polars as pl
df = pl.read_csv("large.csv")  # Loads entire file into memory
```

**After (v0.53.0):**
```python
import polaroid as pl

# Automatic memory management
df = pl.adaptive_scan_csv("large.csv", memory_limit="2GB")

# Or streaming
from polaroid.streaming import CsvSource
source = CsvSource("large.csv", memory_limit="2GB")
for chunk in source:
    process(chunk)
```

#### From Manual S3 Downloads

**Before:**
```python
import boto3
s3 = boto3.client('s3')
s3.download_file('bucket', 'data.parquet', '/tmp/data.parquet')
df = pl.read_parquet('/tmp/data.parquet')
```

**After (v0.53.0):**
```python
from polaroid.streaming import S3Source

# Stream directly from S3
source = S3Source(
    "s3://bucket/data.parquet",
    memory_limit="4GB"
)
for chunk in source:
    process(chunk)
```

#### From Manual API Pagination

**Before:**
```python
import requests
data = []
page = 1
while True:
    r = requests.get(f"https://api.example.com/data?page={page}")
    if not r.json(): break
    data.extend(r.json())
    page += 1
df = pl.DataFrame(data)
```

**After (v0.53.0):**
```python
from polaroid.streaming import HttpSource

# Automatic pagination
source = HttpSource(
    "https://api.example.com/data",
    paginated=True,
    memory_limit="1GB"
)
for chunk in source:
    process(chunk)
```

### Configuration

Create `polaroid.toml` for default settings:

```toml
[sources.csv]
default_memory_limit = "2GB"
default_chunk_size = 10000

[sources.s3]
region = "us-east-1"
default_memory_limit = "4GB"

[sources.http]
timeout = 30
retry_attempts = 3
```

### Memory Recommendations

| Environment | Memory Limit | Chunk Size |
|-------------|--------------|------------|
| Laptop (8GB) | `"2GB"` | 10,000 |
| Desktop (16GB) | `"4GB"` | 50,000 |
| Server (32GB) | `"8GB"` | 100,000 |
| Azure B1s (1GB) | `"400MB"` | 5,000 |
| Azure B2s (4GB) | `"1.5GB"` | 20,000 |

### Custom Sources (Rust)

```rust
use polars_streaming_adaptive::sources::*;
use async_trait::async_trait;

#[derive(Debug)]
struct MySqlSource { /* ... */ }

#[async_trait]
impl StreamingSource for MySqlSource {
    async fn metadata(&self) -> SourceResult<SourceMetadata> { /* ... */ }
    async fn read_chunk(&mut self) -> SourceResult<Option<DataFrame>> { /* ... */ }
    fn stats(&self) -> StreamingStats { /* ... */ }
    async fn reset(&mut self) -> SourceResult<()> { /* ... */ }
    fn has_more(&self) -> bool { /* ... */ }
}

// Register
let mut registry = SourceRegistry::new();
registry.register("mysql", Box::new(MySqlSourceFactory));
```

### See Also

- [API Reference](API_REFERENCE.md) - Complete API documentation
- [User Guide](USER_GUIDE.md) - Comprehensive usage guide
- [Benchmarks](../notebooks/adaptive_streaming_benchmarks.ipynb) - Performance comparisons

---

## üéØ Why Migrate?

### Performance Improvements

- **üåê Remote Execution**: Process data on powerful servers from lightweight clients
- **üìä Streaming-First**: Handle larger-than-RAM datasets with constant memory usage
- **‚ö° Zero-Copy Streaming**: Arrow IPC eliminates serialization overhead
- **üîÑ True Parallelism**: Async Tokio runtime enables concurrent operations
- **üìà Better Scalability**: Multi-client access to shared datasets

### New Features

- **gRPC Architecture**: Handle-based remote execution
- **WebSocket Streaming**: Real-time data ingestion with sub-ms latency
- **REST API Integration**: Built-in pagination and retries
- **Time-Series Native**: OHLCV resampling and rolling windows
- **Distributed Computing**: Process data across multiple nodes
- **Advanced Async**: First-class async/await support

### Use Cases Perfect for Polaroid

‚úÖ Processing datasets larger than your machine's RAM  
‚úÖ Shared data access from multiple clients  
‚úÖ Real-time streaming data pipelines  
‚úÖ Financial data analysis with time-series operations  
‚úÖ Centralized data processing on powerful hardware  
‚úÖ Network-sourced data (WebSocket, REST APIs)

## üîÑ Compatibility Overview

### ‚úÖ What's Compatible

Most Polars operations work identically in Polaroid:

```python
# These work the same in both Polars and Polaroid
df.select(["col1", "col2"])
df.filter(pl.col("price") > 100)
df.group_by("symbol").agg({"price": "mean"})
df.join(df2, on="id", how="inner")
```

### ‚ö†Ô∏è Key Differences

#### 1. **Handle-Based Architecture**

**Polars** (in-memory):
```python
import polars as pl
df = pl.read_parquet("data.parquet")  # Returns DataFrame with data
print(type(df))  # <class 'polars.DataFrame'>
```

**Polaroid** (remote handles):
```python
import polaroid as pd
df = pd.read_parquet("data.parquet")  # Returns Handle reference
print(type(df))  # <class 'polaroid.DataFrame'> (just a UUID)
```

#### 2. **Explicit Collection**

**Polars**:
```python
df = pl.read_parquet("data.parquet")
result = df.select(["col1"])  # Result is immediately available
print(result)  # Prints data
```

**Polaroid**:
```python
df = pd.read_parquet("data.parquet")
df2 = df.select(["col1"])  # Returns new handle
result = df2.collect()  # Explicit collection needed
print(result)  # PyArrow Table
```

#### 3. **Result Types**

**Polars** (raises exceptions):
```python
try:
    df = pl.read_parquet("missing.parquet")
except Exception as e:
    print(f"Error: {e}")
```

**Polaroid** (Result monad):
```python
result = pd.read_parquet("missing.parquet").collect()
if result.is_ok():
    df = result.unwrap()
else:
    error = result.unwrap_err()
```

## üìù Step-by-Step Migration

### 1. Update Dependencies

#### Python

**Before (Polars):**
```toml
# pyproject.toml
dependencies = [
    "polars>=0.19.0",
]
```

**After (Polaroid):**
```toml
# pyproject.toml
dependencies = [
    "polaroid-df>=0.1.0",
]
```

Or with pip:
```bash
pip uninstall polars
pip install polaroid-df
```

#### Rust

**Before (Polars):**
```toml
# Cargo.toml
[dependencies]
polars = "0.35"
```

**After (Polaroid):**
```toml
# Cargo.toml
[dependencies]
polaroid = "0.1"
polaroid-grpc = "0.1"
```

### 2. Update Imports

**Before:**
```python
import polars as pl
from polars import col, lit
```

**After:**
```python
import polaroid as pd
from polaroid import col, lit

# Connect to server
client = pd.connect("localhost:50051")
```

### 3. Add Server Connection

Polaroid requires a running gRPC server:

```bash
# Start server with Docker
docker run -d -p 50051:50051 polaroid/server:latest

# Or build from source
cd polaroid
cargo run -p polaroid-grpc
```

### 4. Update Code Patterns

#### Simple Operations

**Before (Polars):**
```python
df = pl.read_parquet("data.parquet")
result = (
    df
    .filter(pl.col("price") > 100)
    .select(["symbol", "price"])
    .group_by("symbol")
    .agg({"price": "mean"})
)
print(result)
```

**After (Polaroid):**
```python
df = pd.read_parquet("data.parquet")
result = (
    df
    .filter(pd.col("price") > 100)
    .select(["symbol", "price"])
    .group_by("symbol")
    .agg({"price": "mean"})
    .collect()  # Explicit collection
)
print(result)  # PyArrow Table
```

#### Lazy Operations

**Before (Polars):**
```python
lazy_df = pl.scan_parquet("data.parquet")
result = (
    lazy_df
    .filter(pl.col("price") > 100)
    .collect()
)
```

**After (Polaroid):**
```python
# Lazy by default! No need for scan_
df = pd.read_parquet("data.parquet")
result = (
    df
    .filter(pd.col("price") > 100)
    .collect()
)
```

#### Multiple DataFrames

**Before (Polars):**
```python
df1 = pl.read_parquet("data1.parquet")
df2 = pl.read_parquet("data2.parquet")
joined = df1.join(df2, on="id")
```

**After (Polaroid):**
```python
df1 = pd.read_parquet("data1.parquet")
df2 = pd.read_parquet("data2.parquet")
joined = df1.join(df2, on="id")
result = joined.collect()  # Explicit collection
```

### 5. Async Operations (New in Polaroid!)

Polaroid enables true parallel operations:

**Sequential (slow):**
```python
results = []
for i in range(100):
    df = pd.read_parquet(f"file_{i}.parquet")
    result = df.filter(pd.col("value") > 0).collect()
    results.append(result)
```

**Parallel (fast):**
```python
import asyncio

async def process_files():
    async with pd.AsyncClient("localhost:50051") as client:
        # Read all files in parallel
        handles = await asyncio.gather(*[
            client.read_parquet(f"file_{i}.parquet")
            for i in range(100)
        ])
        
        # Process all in parallel
        results = await asyncio.gather(*[
            h.filter(pd.col("value") > 0).collect()
            for h in handles
        ])
    return results

results = await process_files()
```

## üó∫Ô∏è Feature Mapping

### Core Operations

| Operation | Polars | Polaroid | Notes |
|-----------|--------|----------|-------|
| Read Parquet | `pl.read_parquet()` | `pd.read_parquet()` | ‚úÖ Same |
| Read CSV | `pl.read_csv()` | `pd.read_csv()` | ‚úÖ Same |
| Select | `df.select()` | `df.select()` | ‚úÖ Same |
| Filter | `df.filter()` | `df.filter()` | ‚úÖ Same |
| Group By | `df.group_by()` | `df.group_by()` | ‚úÖ Same |
| Join | `df.join()` | `df.join()` | ‚úÖ Same |
| Collect | `df.collect()` (lazy) | `df.collect()` (always) | ‚ö†Ô∏è Always needed |

### I/O Operations

| Operation | Polars | Polaroid | Notes |
|-----------|--------|----------|-------|
| Read local file | `pl.read_parquet("file.parquet")` | `pd.read_parquet("file.parquet")` | ‚úÖ Same |
| Scan lazy | `pl.scan_parquet()` | `pd.read_parquet()` | ‚ö†Ô∏è Lazy by default |
| Write Parquet | `df.write_parquet()` | `df.write_parquet()` | ‚úÖ Same |
| Write CSV | `df.write_csv()` | `df.write_csv()` | ‚úÖ Same |

### Expressions

| Operation | Polars | Polaroid | Notes |
|-----------|--------|----------|-------|
| Column reference | `pl.col("name")` | `pd.col("name")` | ‚úÖ Same |
| Literal value | `pl.lit(100)` | `pd.lit(100)` | ‚úÖ Same |
| String ops | `.str.contains()` | `.str.contains()` | ‚úÖ Same |
| Datetime ops | `.dt.year()` | `.dt.year()` | ‚úÖ Same |
| Aggregations | `.mean()`, `.sum()` | `.mean()`, `.sum()` | ‚úÖ Same |

### New in Polaroid

| Operation | Polars | Polaroid | Notes |
|-----------|--------|----------|-------|
| Async client | ‚ùå N/A | `pd.AsyncClient()` | ‚ú® New |
| WebSocket | ‚ùå N/A | `pd.from_websocket()` | ‚ú® New |
| REST API | ‚ùå N/A | `pd.read_rest_api()` | ‚ú® New |
| Time-series | Manual | `df.as_timeseries()` | ‚ú® New |
| OHLCV resample | Manual | `.resample_ohlcv()` | ‚ú® New |
| Result monad | Exceptions | `.is_ok()`, `.unwrap()` | ‚ú® New |

## üß™ Testing Your Migration

### Unit Testing Strategy

**Before (Polars):**
```python
def test_data_processing():
    df = pl.read_parquet("test_data.parquet")
    result = df.filter(pl.col("value") > 0)
    assert result.shape[0] > 0
```

**After (Polaroid):**
```python
def test_data_processing():
    df = pd.read_parquet("test_data.parquet")
    result = df.filter(pd.col("value") > 0).collect()
    assert result.num_rows > 0
```

### Performance Benchmarking

```python
import time

# Polars
start = time.time()
df = pl.read_parquet("data.parquet")
result = df.filter(pl.col("price") > 100)
polars_time = time.time() - start

# Polaroid
start = time.time()
df = pd.read_parquet("data.parquet")
result = df.filter(pd.col("price") > 100).collect()
polaroid_time = time.time() - start

print(f"Polars: {polars_time:.3f}s")
print(f"Polaroid: {polaroid_time:.3f}s")
print(f"Overhead: {((polaroid_time/polars_time)-1)*100:.1f}%")
```

**Expected Results:**
- Small datasets (<100MB): 5-10% overhead (gRPC network cost)
- Large datasets (>1GB): Similar or better performance
- Streaming (>RAM): Polaroid wins (Polars OOM)
- Concurrent operations: Polaroid 10-100x faster

### Validation Checklist

- [ ] All imports updated (`polars` ‚Üí `polaroid`)
- [ ] Server connection established
- [ ] `.collect()` added where needed
- [ ] Result types handled (`.unwrap()` or error checks)
- [ ] Async operations converted to `AsyncClient`
- [ ] Tests updated and passing
- [ ] Performance benchmarks acceptable
- [ ] Memory usage checked for large datasets

## üêõ Common Migration Issues

### Issue 1: Missing `.collect()`

**Error:**
```python
df = pd.read_parquet("data.parquet")
print(df)  # Prints: DataFrame(handle="abc-123")
```

**Solution:**
```python
df = pd.read_parquet("data.parquet")
result = df.collect()
print(result)  # Prints: pyarrow.Table
```

### Issue 2: Immediate Evaluation Expected

**Error:**
```python
df = pd.read_parquet("data.parquet")
print(df.shape)  # AttributeError: 'DataFrame' object has no attribute 'shape'
```

**Solution:**
```python
df = pd.read_parquet("data.parquet")
result = df.collect()
print(result.num_rows, result.num_columns)
```

### Issue 3: Exception Handling

**Before:**
```python
try:
    df = pl.read_parquet("data.parquet")
except FileNotFoundError:
    print("File not found")
```

**After:**
```python
result = pd.read_parquet("data.parquet").collect()
if result.is_err():
    print(f"Error: {result.unwrap_err()}")
```

### Issue 4: Type Differences

**Polars** returns `polars.DataFrame`, **Polaroid** returns `pyarrow.Table`:

```python
# Polars
df = pl.read_parquet("data.parquet")
print(type(df))  # <class 'polars.DataFrame'>

# Polaroid
result = pd.read_parquet("data.parquet").collect()
print(type(result))  # <class 'pyarrow.Table'>

# Convert to Pandas if needed
pandas_df = result.to_pandas()
```

## üîß Migration Tools

### Automated Import Replacement

```bash
# Find all Polars imports
find . -name "*.py" -exec grep -l "import polars" {} \;

# Replace imports (macOS)
find . -name "*.py" -exec sed -i '' 's/import polars as pl/import polaroid as pd/g' {} \;

# Replace imports (Linux)
find . -name "*.py" -exec sed -i 's/import polars as pl/import polaroid as pd/g' {} \;
```

### Linting Rules

Add to your `.pylintrc`:

```ini
[SIMILARITIES]
# Detect Polars usage in Polaroid projects
check-polars-imports=true
```

### Pre-commit Hook

```yaml
# .pre-commit-config.yaml
repos:
  - repo: local
    hooks:
      - id: check-polars-imports
        name: Check for Polars imports
        entry: bash -c 'if grep -r "import polars" .; then exit 1; fi'
        language: system
        pass_filenames: false
```

## üìö Case Study: Real Migration

### Small Project (500 lines)

**Project:** Data analysis script

**Changes needed:**
1. Update imports (5 minutes)
2. Add `.collect()` calls (10 minutes)
3. Update tests (15 minutes)
4. Performance validation (10 minutes)

**Total time:** ~40 minutes

**Result:** +7% overhead, but gained streaming capability

### Large Codebase (50K lines)

**Project:** Financial data platform

**Strategy:**
1. **Phase 1:** Migrate read/write operations (week 1)
2. **Phase 2:** Migrate transformations (week 2-3)
3. **Phase 3:** Add async operations (week 4)
4. **Phase 4:** Enable streaming for large datasets (week 5)

**Challenges:**
- 200+ files to update
- Complex test suite
- Performance regression concerns

**Solutions:**
- Automated import replacement
- Incremental migration by module
- Parallel testing (Polars vs Polaroid)
- Gradual rollout with feature flags

**Results:**
- Migration completed in 5 weeks
- 15% performance improvement on large datasets
- 50% memory reduction for streaming workflows
- 20x speedup for concurrent operations

## üÜò Getting Help

### Community Support

- **GitHub Issues**: https://github.com/EnkiNudimmud/polaroid/issues
- **Discussions**: https://github.com/EnkiNudimmud/polaroid/discussions
- **Discord**: [Join our community](#)

### Migration Assistance

Need help with migration? Open an issue with:
- Your Polars code snippet
- Expected behavior
- Any errors encountered
- Performance requirements

We'll help you find the best Polaroid equivalent!

---

**See Also**:
- [Quick Reference](QUICK_REFERENCE.md) - Common operations cheat sheet
- [API Documentation](API_DOCUMENTATION.md) - Complete API reference
- [Architecture Guide](ARCHITECTURE.md) - Understanding Polaroid's design
